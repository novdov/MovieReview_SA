{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(doc, stopwords):\n",
    "    \"\"\"\n",
    "    Remove stopwords from doc.\n",
    "    :param doc: Document to remove stopwords\n",
    "    :param stopwords: stopwords corpus\n",
    "    :return: documents with removed stopwords\n",
    "    \"\"\"\n",
    "    stopwords_removed = []\n",
    "    for token in doc:\n",
    "        if not token in stopwords:\n",
    "            stopwords_removed.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_weights(matrix, x1, x2):\n",
    "    \"\"\"\n",
    "    Get normalized weights matrix. Used for np.dot(TDM, weights).\n",
    "    :param matrix: similarity matrix (euclidean or cosine)\n",
    "    :param x1: rows\n",
    "    :param x2: cols\n",
    "    :return: normalized weights matrix\n",
    "    \"\"\"\n",
    "    distance = matrix[x1, x2] ** 2\n",
    "    variance = np.var(matrix)\n",
    "    weights = np.exp(-(distance / (2 * variance ** 2)))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = ['귀신/Noun', '깜놀/Noun', '놀라다/Verb', '멀미/Noun', '놀램/Noun',\n",
    "       '지르다/Verb', '갑툭튀/Noun', '지름/Noun', '놀람/Noun', '나쁘다/Adjective',\n",
    "       '비명/Noun', '튀어나오다/Verb', '소리내다/Verb', '오지/Noun', '울다/Verb',\n",
    "       '놀랬/Noun', '무섭다/Adjective', '깜짝/Noun', '놀란/Noun', '놀랬어/Noun', \n",
    "       '깜짝깜짝/Adverb', '섬뜩/Adverb', '소름/Noun', '돋다/Adjective']\n",
    "\n",
    "li = remove_pos(tmp)\n",
    "\n",
    "def get_similar_words(docs):\n",
    "    li_new = []\n",
    "    for word in li:\n",
    "        try:\n",
    "            token = model.wv.most_similar(tokenize(twitter, word)[0], topn=8)\n",
    "            if token not in li_new:\n",
    "                li_new.append(token)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return li_new\n",
    "\n",
    "li_new = get_similar_words(li)\n",
    "for line in li_new:\n",
    "    for w in line:\n",
    "        if w[0] not in tmp:\n",
    "            tmp.append(w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = read_data('../reviews/all_contents_03.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 33s, sys: 1.34 s, total: 4min 34s\n",
      "Wall time: 4min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_docs = [tokenize(row[0]) for row in train_data]\n",
    "train_docs_no_pos = [remove_pos(docs) for docs in train_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['힘내다/Verb',\n",
      " '모두/Noun',\n",
      " '들/Suffix',\n",
      " '!!/Punctuation',\n",
      " '진실/Noun',\n",
      " '을/Josa',\n",
      " '밝히다/Verb',\n",
      " '!!/Punctuation']\n",
      "\n",
      "['힘내다', '모두', '들', '!!', '진실', '을', '밝히다', '!!']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(train_docs[-100])\n",
    "print('')\n",
    "pprint(train_docs_no_pos[-100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: NMSC>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "tokens = [t for d in train_docs for t in d]\n",
    "text = nltk.Text(tokens, name=\"NMSC\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3931769\n",
      "63779\n",
      "[('./Punctuation', 115283),\n",
      " ('하다/Verb', 93744),\n",
      " ('영화/Noun', 90605),\n",
      " ('이/Josa', 71434),\n",
      " ('보다/Verb', 65114),\n",
      " ('의/Josa', 51345),\n",
      " ('에/Josa', 49016),\n",
      " ('../Punctuation', 47244),\n",
      " ('가/Josa', 46659),\n",
      " ('을/Josa', 42707)]\n"
     ]
    }
   ],
   "source": [
    "print(len(text.tokens))\n",
    "print(len(set(text.tokens)))\n",
    "pprint(text.vocab().most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 58s, sys: 3.74 s, total: 17min 2s\n",
      "Wall time: 4min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(train_docs, size=300, workers=4, min_count=10, iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_pickle('../train_docs_0426.pickle', train_docs)\n",
    "save_pickle('../train_docs_no_pos_0426.pickle', train_docs_no_pos)\n",
    "save_pickle('../nltk_text_0426.pickle', text)\n",
    "model.save('../model/review_word2vec_20180426')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter; twitter = Twitter()\n",
    "from gensim.models import word2vec\n",
    "train_docs = load_pickle('../train_docs_0426.pickle')\n",
    "train_docs_no_pos = load_pickle('../train_docs_no_pos_0426.pickle')\n",
    "text = load_pickle('../nltk_text_0426.pickle')\n",
    "model = word2vec.Word2Vec.load('../model/review_word2vec_20180426')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['기쁘다/Adjective',\n",
       " '화나다/Verb',\n",
       " '역겹다/Adjective',\n",
       " '슬프다/Adjective',\n",
       " '무섭다/Adjective']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_pair = {'joy': '기쁘다', 'anger': '화나다', \n",
    "                'disgust': '역겹다', 'sadness': '슬프다', 'fear': '무섭다'}\n",
    "emotion_ko_list = list(emotion_pair.values())\n",
    "emotion_ko_dic = {\n",
    "    '기쁘다': 0, \n",
    "    '화나다': 1, \n",
    "    '역겹다': 2,\n",
    "    '슬프다': 3,\n",
    "    '무섭다': 4,\n",
    "}\n",
    "emotion_ko_list = [tokenize(row) for row in emotion_ko_list]\n",
    "emotion_ko_list = [row[0] for row in emotion_ko_list]\n",
    "emotion_ko_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('무섭다/Adjective', 0.6415336728096008),\n",
       " ('애나벨/Noun', 0.36187833547592163),\n",
       " ('귀신/Noun', 0.3564368188381195),\n",
       " ('깜놀/Noun', 0.3522293269634247),\n",
       " ('놀램/Noun', 0.3495345115661621)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('무서움/Noun', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def read_data_comma(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = [line.split(',') for line in f.read().splitlines()]\n",
    "        data = data[1:]\n",
    "    return data\n",
    "\n",
    "# emotion_corpus = read_data_comma('../emotion_corpus_labeled.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 154, 139, 95, 88)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_corpus = pd.read_csv('../emotion_corpus_labeled.txt', encoding='utf-8', sep=',')\n",
    "\n",
    "joy_list = emotion_corpus[emotion_corpus['emotion']==0].values.tolist()\n",
    "anger_list = emotion_corpus[emotion_corpus['emotion']==1].values.tolist()\n",
    "disgust_list = emotion_corpus[emotion_corpus['emotion']==2].values.tolist()\n",
    "sadness_list = emotion_corpus[emotion_corpus['emotion']==3].values.tolist()\n",
    "fear_list = emotion_corpus[emotion_corpus['emotion']==4].values.tolist()\n",
    "\n",
    "joy_list = [row[0] for row in joy_list]\n",
    "anger_list = [row[0] for row in anger_list]\n",
    "disgust_list = [row[0] for row in disgust_list]\n",
    "sadness_list = [row[0] for row in sadness_list]\n",
    "fear_list = [row[0] for row in fear_list]\n",
    "\n",
    "len(joy_list), len(anger_list), len(disgust_list), len(sadness_list), len(fear_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.4 s, sys: 97.3 ms, total: 31.5 s\n",
      "Wall time: 31.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_docs_labeled = []\n",
    "\n",
    "for row in train_docs:\n",
    "    joy_score = 0\n",
    "    anger_score = 0\n",
    "    disgust_score = 0\n",
    "    sadness_score = 0\n",
    "    fear_score = 0\n",
    "    all_scores = {}\n",
    "    \n",
    "    all_scores['기쁘다'] = joy_score\n",
    "    all_scores['화나다'] = anger_score\n",
    "    all_scores['역겹다'] = disgust_score\n",
    "    all_scores['슬프다'] = sadness_score\n",
    "    all_scores['무섭다'] = fear_score\n",
    "    \n",
    "    for _, token in enumerate(row):\n",
    "            \n",
    "        if token in joy_list:\n",
    "            joy_score += 1\n",
    "        elif token in anger_list:\n",
    "            anger_score += 1\n",
    "        elif token in disgust_list:\n",
    "            disgust_score += 1\n",
    "        elif token in sadness_list:\n",
    "            sadness_score += 1\n",
    "        elif token in fear_list:\n",
    "            fear_score += 1\n",
    "        \n",
    "        all_scores['기쁘다'] += joy_score\n",
    "        all_scores['화나다'] += anger_score\n",
    "        all_scores['역겹다'] += disgust_score\n",
    "        all_scores['슬프다'] += sadness_score\n",
    "        all_scores['무섭다'] += fear_score   \n",
    "        \n",
    "        label = max(all_scores, key=lambda key: all_scores[key])\n",
    "    \n",
    "    if all_scores[label] == 0:\n",
    "        label = '중립'\n",
    "        train_docs_labeled.append((row, label))\n",
    "#     elif all_scores['기쁘다'] == all_scores['화나다'] == all_scores['역겹다'] == all_scores['슬프다'] == all_scores['무섭다']:\n",
    "#         label = '중립'\n",
    "#         train_docs_labeled.append((row, label))\n",
    "    else:\n",
    "        train_docs_labeled.append((row, label))   \n",
    "#     print(all_scores)\n",
    "#     print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'중립': 96261,\n",
      "         '기쁘다': 78493,\n",
      "         '화나다': 28464,\n",
      "         '역겹다': 18696,\n",
      "         '슬프다': 17086,\n",
      "         '무섭다': 13847})\n",
      "\n",
      "무섭다 : 0.05\n",
      "중립 : 0.38\n",
      "역겹다 : 0.07\n",
      "슬프다 : 0.07\n",
      "기쁘다 : 0.31\n",
      "화나다 : 0.11\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "all_labeled = [row[1] for row in train_docs_labeled]\n",
    "\n",
    "from collections import Counter\n",
    "label_count = Counter(all_labeled)\n",
    "pprint(label_count)\n",
    "print()\n",
    "sum_ = sum(label_count.values())\n",
    "for label in label_count.keys():\n",
    "    print(label ,\":\", round(label_count[label] / sum_, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2018/04/28 15:18**\n",
    "- 중립이 생각보다 많다. (38%)\n",
    "- 긍정/부정/중립 비율은 31:31:38 (sentiment 상으로는 비율이 맞게 보임)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96261\n"
     ]
    }
   ],
   "source": [
    "train_docs_labeld_neutral = []\n",
    "for row in train_docs_labeled:\n",
    "    if row[1] == '중립':\n",
    "        train_docs_labeld_neutral.append(row)\n",
    "\n",
    "indicies = []\n",
    "\n",
    "for idx, row in enumerate(train_docs_labeled):\n",
    "    if row[1] == '중립':\n",
    "        indicies.append(idx)\n",
    "print(len(indicies))\n",
    "train_docs_neutral = np.array(train_docs)[indicies].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**어떤 리뷰가 중립으로 판별되었나?**\n",
    "1. 다섯 가지 감정으로 분류하기 어려운 리뷰 (***회의*** --> ???)\n",
    "2. 리뷰만으로는 긍정/부정을 알기 어려운 리뷰 (***더잘어울리는*** --> ??)\n",
    "3. 감정을 나타내는 어휘가 없는 리뷰\n",
    "\n",
    "**해결 방안**\n",
    "- 1, 3: 제외\n",
    "- 2: 긍정/부정 레이블 확인 or 평점 확인 --> 문제점: 레이블이 없는 리뷰도 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"가이 리치의 영화, 1998년 록 스탁 앤 투 스모킹 배럴즈를 좋아하는 저는, 글쎄요, 극적 설정과 줄거리에 회의가 들어요.\"']\n",
      "['무법자의 천상천하유아독쫑 이란 제목이 더 잘어울리는 영화!']\n",
      "['조조 히데오감독은 나름 내용에 의미는 부여 하는 듯!']\n",
      "['명대사: 앤딩부분 여선생曰 적당']\n"
     ]
    }
   ],
   "source": [
    "print(train_data[indicies[0]])\n",
    "print(train_data[indicies[10]])\n",
    "print(train_data[indicies[25]])\n",
    "print(train_data[indicies[52]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 14s, sys: 1.27 s, total: 4min 15s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_neutral = word2vec.Word2Vec(train_docs_neutral, size=300, workers=4, min_count=10, iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_pickle('../train_docs_neutral_0427.pickle', train_docs_neutral)\n",
    "model_neutral.save('../model/review_word2vec_neutral_20180427')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(model_neutral.wv.vocab.keys()), columns=['token'])\n",
    "df.to_csv('../neutral_corpus.txt', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "text_neutral = nltk.Text(train_docs_neutral, name=\"NEUTRAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['싸구려/Noun']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('싸구려')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('삼류/Noun', 0.33203670382499695),\n",
       " ('신파극/Noun', 0.3291999399662018),\n",
       " ('저급/Noun', 0.3059639632701874),\n",
       " ('에로영화/Noun', 0.291871041059494),\n",
       " ('에로티시즘/Noun', 0.28579774498939514),\n",
       " ('비급/Noun', 0.282213419675827),\n",
       " ('에로물/Noun', 0.2783181369304657),\n",
       " ('헐리우드/Noun', 0.27360162138938904),\n",
       " ('질/Noun', 0.2633456289768219),\n",
       " ('무협지/Noun', 0.2590341866016388),\n",
       " ('단순하다/Adjective', 0.25870680809020996),\n",
       " ('양산/Noun', 0.25715211033821106),\n",
       " ('억지/Noun', 0.25333061814308167),\n",
       " ('쓰레기/Noun', 0.2532225549221039),\n",
       " ('요즘/Noun', 0.2517792582511902),\n",
       " ('류/Noun', 0.25019702315330505),\n",
       " ('시답/Noun', 0.25014448165893555),\n",
       " ('어설픈/Noun', 0.2495957314968109),\n",
       " ('포르노/Noun', 0.24708467721939087),\n",
       " ('코미디/Noun', 0.2448120415210724),\n",
       " ('뭣/Noun', 0.24169448018074036),\n",
       " ('경박/Noun', 0.23776760697364807),\n",
       " ('설탕/Noun', 0.2366945594549179),\n",
       " ('애로/Noun', 0.23624417185783386),\n",
       " ('천박/Noun', 0.233965665102005),\n",
       " ('코메디/Noun', 0.23341765999794006),\n",
       " ('썰렁하다/Adjective', 0.2324945479631424),\n",
       " ('유아/Noun', 0.23086710274219513),\n",
       " ('옴니버스/Noun', 0.22778049111366272),\n",
       " ('신파/Noun', 0.22747652232646942),\n",
       " ('잡탕/Noun', 0.2268172651529312),\n",
       " ('예찬/Noun', 0.225499227643013),\n",
       " ('상업/Noun', 0.2249019593000412),\n",
       " ('고급/Noun', 0.2247728556394577),\n",
       " ('막장/Noun', 0.22240526974201202),\n",
       " ('우리나라/Noun', 0.22227269411087036),\n",
       " ('놨/Noun', 0.22185513377189636),\n",
       " ('급/Noun', 0.22085027396678925),\n",
       " ('고풍/Noun', 0.220540389418602),\n",
       " ('장편/Noun', 0.21950536966323853),\n",
       " ('퍼레이드/Noun', 0.21818757057189941),\n",
       " ('S/Alpha', 0.21727009117603302),\n",
       " ('최고급/Noun', 0.216452494263649),\n",
       " ('개똥/Noun', 0.2163817584514618),\n",
       " ('황금기/Noun', 0.21604424715042114),\n",
       " ('특촬물/Noun', 0.21576321125030518),\n",
       " ('보급/Noun', 0.2140689492225647),\n",
       " ('전락/Noun', 0.21236860752105713),\n",
       " ('아동/Noun', 0.21089813113212585),\n",
       " ('맛없다/Adjective', 0.21064603328704834)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('싸구려/Noun', topn=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

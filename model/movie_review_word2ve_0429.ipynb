{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(doc, stopwords):\n",
    "    \"\"\"\n",
    "    Remove stopwords from doc.\n",
    "    :param doc: Document to remove stopwords\n",
    "    :param stopwords: stopwords corpus\n",
    "    :return: documents with removed stopwords\n",
    "    \"\"\"\n",
    "    stopwords_removed = []\n",
    "    for token in doc:\n",
    "        if not token in stopwords:\n",
    "            stopwords_removed.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_weights(matrix, x1, x2):\n",
    "    \"\"\"\n",
    "    Get normalized weights matrix. Used for np.dot(TDM, weights).\n",
    "    :param matrix: similarity matrix (euclidean or cosine)\n",
    "    :param x1: rows\n",
    "    :param x2: cols\n",
    "    :return: normalized weights matrix\n",
    "    \"\"\"\n",
    "    distance = matrix[x1, x2] ** 2\n",
    "    variance = np.var(matrix)\n",
    "    weights = np.exp(-(distance / (2 * variance ** 2)))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = ['귀신/Noun', '깜놀/Noun', '놀라다/Verb', '멀미/Noun', '놀램/Noun',\n",
    "       '지르다/Verb', '갑툭튀/Noun', '지름/Noun', '놀람/Noun', '나쁘다/Adjective',\n",
    "       '비명/Noun', '튀어나오다/Verb', '소리내다/Verb', '오지/Noun', '울다/Verb',\n",
    "       '놀랬/Noun', '무섭다/Adjective', '깜짝/Noun', '놀란/Noun', '놀랬어/Noun', \n",
    "       '깜짝깜짝/Adverb', '섬뜩/Adverb', '소름/Noun', '돋다/Adjective']\n",
    "\n",
    "li = remove_pos(tmp)\n",
    "\n",
    "def get_similar_words(docs):\n",
    "    li_new = []\n",
    "    for word in li:\n",
    "        try:\n",
    "            token = model.wv.most_similar(tokenize(twitter, word)[0], topn=8)\n",
    "            if token not in li_new:\n",
    "                li_new.append(token)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return li_new\n",
    "\n",
    "li_new = get_similar_words(li)\n",
    "for line in li_new:\n",
    "    for w in line:\n",
    "        if w[0] not in tmp:\n",
    "            tmp.append(w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = read_data('../reviews/all_contents_03.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 33s, sys: 1.34 s, total: 4min 34s\n",
      "Wall time: 4min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_docs = [tokenize(row[0]) for row in train_data]\n",
    "train_docs_no_pos = [remove_pos(docs) for docs in train_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['힘내다/Verb',\n",
      " '모두/Noun',\n",
      " '들/Suffix',\n",
      " '!!/Punctuation',\n",
      " '진실/Noun',\n",
      " '을/Josa',\n",
      " '밝히다/Verb',\n",
      " '!!/Punctuation']\n",
      "\n",
      "['힘내다', '모두', '들', '!!', '진실', '을', '밝히다', '!!']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(train_docs[-100])\n",
    "print('')\n",
    "pprint(train_docs_no_pos[-100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: NMSC>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "tokens = [t for d in train_docs for t in d]\n",
    "text = nltk.Text(tokens, name=\"NMSC\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3931769\n",
      "63779\n",
      "[('./Punctuation', 115283),\n",
      " ('하다/Verb', 93744),\n",
      " ('영화/Noun', 90605),\n",
      " ('이/Josa', 71434),\n",
      " ('보다/Verb', 65114),\n",
      " ('의/Josa', 51345),\n",
      " ('에/Josa', 49016),\n",
      " ('../Punctuation', 47244),\n",
      " ('가/Josa', 46659),\n",
      " ('을/Josa', 42707)]\n"
     ]
    }
   ],
   "source": [
    "print(len(text.tokens))\n",
    "print(len(set(text.tokens)))\n",
    "pprint(text.vocab().most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 58s, sys: 3.74 s, total: 17min 2s\n",
      "Wall time: 4min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(train_docs, size=300, workers=4, min_count=10, iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_pickle('../train_docs_0426.pickle', train_docs)\n",
    "save_pickle('../train_docs_no_pos_0426.pickle', train_docs_no_pos)\n",
    "save_pickle('../nltk_text_0426.pickle', text)\n",
    "model.save('../model/review_word2vec_20180426')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter; twitter = Twitter()\n",
    "from gensim.models import word2vec\n",
    "train_docs = load_pickle('../train_docs_0426.pickle')\n",
    "train_docs_no_pos = load_pickle('../train_docs_no_pos_0426.pickle')\n",
    "text = load_pickle('../nltk_text_0426.pickle')\n",
    "model = word2vec.Word2Vec.load('../model/review_word2vec_20180426')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['기쁘다/Adjective',\n",
       " '화나다/Verb',\n",
       " '역겹다/Adjective',\n",
       " '슬프다/Adjective',\n",
       " '무섭다/Adjective']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_pair = {'joy': '기쁘다', 'anger': '화나다', \n",
    "                'disgust': '역겹다', 'sadness': '슬프다', 'fear': '무섭다'}\n",
    "emotion_ko_list = list(emotion_pair.values())\n",
    "emotion_ko_dic = {\n",
    "    '기쁘다': 0, \n",
    "    '화나다': 1, \n",
    "    '역겹다': 2,\n",
    "    '슬프다': 3,\n",
    "    '무섭다': 4,\n",
    "}\n",
    "emotion_ko_list = [tokenize(row) for row in emotion_ko_list]\n",
    "emotion_ko_list = [row[0] for row in emotion_ko_list]\n",
    "emotion_ko_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('무섭다/Adjective', 0.6415336728096008),\n",
       " ('애나벨/Noun', 0.36187833547592163),\n",
       " ('귀신/Noun', 0.3564368188381195),\n",
       " ('깜놀/Noun', 0.3522293269634247),\n",
       " ('놀램/Noun', 0.3495345115661621)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('무서움/Noun', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def read_data_comma(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = [line.split(',') for line in f.read().splitlines()]\n",
    "        data = data[1:]\n",
    "    return data\n",
    "\n",
    "# emotion_corpus = read_data_comma('../emotion_corpus_labeled.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174, 196, 179, 154, 106)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_corpus = pd.read_csv('../emotion_corpus_labeled.txt', encoding='utf-8', sep=',')\n",
    "\n",
    "joy_list = emotion_corpus[emotion_corpus['emotion']==0].values.tolist()\n",
    "anger_list = emotion_corpus[emotion_corpus['emotion']==1].values.tolist()\n",
    "disgust_list = emotion_corpus[emotion_corpus['emotion']==2].values.tolist()\n",
    "sadness_list = emotion_corpus[emotion_corpus['emotion']==3].values.tolist()\n",
    "fear_list = emotion_corpus[emotion_corpus['emotion']==4].values.tolist()\n",
    "\n",
    "joy_list = [row[0] for row in joy_list]\n",
    "anger_list = [row[0] for row in anger_list]\n",
    "disgust_list = [row[0] for row in disgust_list]\n",
    "sadness_list = [row[0] for row in sadness_list]\n",
    "fear_list = [row[0] for row in fear_list]\n",
    "\n",
    "len(joy_list), len(anger_list), len(disgust_list), len(sadness_list), len(fear_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.8 s, sys: 614 ms, total: 41.4 s\n",
      "Wall time: 41.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_docs_labeled = []\n",
    "\n",
    "for row in train_docs:\n",
    "    joy_score = 0\n",
    "    anger_score = 0\n",
    "    disgust_score = 0\n",
    "    sadness_score = 0\n",
    "    fear_score = 0\n",
    "    all_scores = {}\n",
    "    \n",
    "    all_scores['기쁘다'] = joy_score\n",
    "    all_scores['화나다'] = anger_score\n",
    "    all_scores['역겹다'] = disgust_score\n",
    "    all_scores['슬프다'] = sadness_score\n",
    "    all_scores['무섭다'] = fear_score\n",
    "    \n",
    "    for _, token in enumerate(row):\n",
    "            \n",
    "        if token in joy_list:\n",
    "            joy_score += 1\n",
    "        elif token in anger_list:\n",
    "            anger_score += 1\n",
    "        elif token in disgust_list:\n",
    "            disgust_score += 1\n",
    "        elif token in sadness_list:\n",
    "            sadness_score += 1\n",
    "        elif token in fear_list:\n",
    "            fear_score += 1\n",
    "        \n",
    "        all_scores['기쁘다'] += joy_score\n",
    "        all_scores['화나다'] += anger_score\n",
    "        all_scores['역겹다'] += disgust_score\n",
    "        all_scores['슬프다'] += sadness_score\n",
    "        all_scores['무섭다'] += fear_score   \n",
    "        \n",
    "        label = max(all_scores, key=lambda key: all_scores[key])\n",
    "    \n",
    "    if all_scores[label] == 0:\n",
    "        label = '중립'\n",
    "        train_docs_labeled.append((row, label))\n",
    "    elif all_scores['기쁘다'] == all_scores['화나다'] == all_scores['역겹다'] == all_scores['슬프다'] == all_scores['무섭다']:\n",
    "        label = '중립'\n",
    "        train_docs_labeled.append((row, label))\n",
    "    else:\n",
    "        train_docs_labeled.append((row, label))   \n",
    "#     print(all_scores)\n",
    "#     print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'기쁘다': 80251,\n",
      "         '중립': 77900,\n",
      "         '화나다': 41013,\n",
      "         '슬프다': 20687,\n",
      "         '역겹다': 20376,\n",
      "         '무섭다': 12620})\n",
      "\n",
      "무섭다 : 0.05\n",
      "중립 : 0.31\n",
      "역겹다 : 0.08\n",
      "슬프다 : 0.08\n",
      "기쁘다 : 0.32\n",
      "화나다 : 0.16\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "all_labeled = [row[1] for row in train_docs_labeled]\n",
    "\n",
    "from collections import Counter\n",
    "label_count = Counter(all_labeled)\n",
    "pprint(label_count)\n",
    "print()\n",
    "sum_ = sum(label_count.values())\n",
    "for label in label_count.keys():\n",
    "    print(label ,\":\", round(label_count[label] / sum_, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2018/04/28 15:18**\n",
    "- 중립이 생각보다 많다. (38%)\n",
    "- 긍정/부정/중립 비율은 31:31:38 (sentiment 상으로는 비율이 맞게 보임)\n",
    "\n",
    "**2018/04/28 21:36**\n",
    "- 긍정/부정/중립 비율은 32:37:31 (무섭다의 비율 감소)\n",
    "- 무섭다의 비율이 적은 건 공표영화의 수가 적어서인 것으로 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77900\n"
     ]
    }
   ],
   "source": [
    "train_docs_labeld_neutral = []\n",
    "for row in train_docs_labeled:\n",
    "    if row[1] == '중립':\n",
    "        train_docs_labeld_neutral.append(row)\n",
    "\n",
    "indicies = []\n",
    "\n",
    "for idx, row in enumerate(train_docs_labeled):\n",
    "    if row[1] == '중립':\n",
    "        indicies.append(idx)\n",
    "print(len(indicies))\n",
    "train_docs_neutral = np.array(train_docs)[indicies].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2018/04/28 15:18**<br>\n",
    "**어떤 리뷰가 중립으로 판별되었나?**\n",
    "1. 다섯 가지 감정으로 분류하기 어려운 리뷰 (***회의*** --> ???)\n",
    "2. 리뷰만으로는 긍정/부정을 알기 어려운 리뷰 (***더잘어울리는*** --> ??)\n",
    "3. 감정을 나타내는 어휘가 없는 리뷰\n",
    "\n",
    "**해결 방안**\n",
    "- 1, 3: 제외\n",
    "- 2: 긍정/부정 레이블 확인 or 평점 확인 --> 문제점: 레이블이 없는 리뷰도 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"가이 리치의 영화, 1998년 록 스탁 앤 투 스모킹 배럴즈를 좋아하는 저는, 글쎄요, 극적 설정과 줄거리에 회의가 들어요.\"']\n",
      "['법 이란걸 집행하는사람이 범죄자들에게 조종당하는 현실을 비판하는 영화']\n",
      "['코난 시리즈중 침묵의15분다음으로 볼만했어요!우리꼬맹이왈']\n",
      "['\"스토리, 연출, 연기, 비주얼 등 영화의 기본 조차 안된 영화에 무슨 평을 해. 이런 영화 찍고도 김문옥 감독은 내가 영화 경력이 몇OO인데 조무래기들이 내 영화를 평론해? 같은 마인드에 빠져있겠지?\"']\n"
     ]
    }
   ],
   "source": [
    "print(train_data[indicies[0]])\n",
    "print(train_data[indicies[10]])\n",
    "print(train_data[indicies[25]])\n",
    "print(train_data[indicies[52]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 8s, sys: 901 ms, total: 3min 9s\n",
      "Wall time: 57.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_neutral = word2vec.Word2Vec(train_docs_neutral, size=300, workers=4, min_count=10, iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_pickle('../train_docs_neutral_0428_2136.pickle', train_docs_neutral)\n",
    "model_neutral.save('../model/review_word2vec_neutral_20180428_2136')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(model_neutral.wv.vocab.keys()), columns=['token'])\n",
    "df.to_csv('../neutral_corpus_0428.txt', index=False, encoding='utf-8')\n",
    "train_data_neutral = pd.DataFrame(np.array(train_data)[indicies].tolist(), columns=['sentence'])\n",
    "train_data_neutral.head()\n",
    "train_data_neutral.to_csv('../neutral_data_0428.txt', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "text_neutral = nltk.Text(train_docs_neutral, name=\"NEUTRAL\")\n",
    "save_pickle('../nltk_text_neutral.picke' , text_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle('../train_docs_lebeled.pickle', train_docs_labeled)\n",
    "save_pickle('../train_data_lebeled.pickle', train_data_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emotion_label = [row[1] for row in train_docs_labeled]\n",
    "train_data_labeled = [([train_data[i][0]], emotion_label[i]) for i in range(len(emotion_label))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['\"75분짜리, 결코  길다할 수없는 애니메이션 한편에 이정도 탄탄한 내용을 담을 수 있단 사실에 정말 놀랐다\"'], '무섭다')"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_labeled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_labeled_joy = [row[0] for row in train_data_labeled if row[1] == '기쁘다']\n",
    "train_data_labeled_anger = [row[0] for row in train_data_labeled if row[1] == '화나다']\n",
    "train_data_labeled_disgust = [row[0] for row in train_data_labeled if row[1] == '역겹다']\n",
    "train_data_labeled_sadness = [row[0] for row in train_data_labeled if row[1] == '슬프다']\n",
    "train_data_labeled_fear = [row[0] for row in train_data_labeled if row[1] == '무섭다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174947 174947\n"
     ]
    }
   ],
   "source": [
    "train_data_labeled_no_neutral = [row for row in train_data_labeled if row[1] != '중립']\n",
    "train_docs_labeled_no_neutral = [row for row in train_docs_labeled if row[1] != '중립']\n",
    "print(len(train_data_labeled_no_neutral), len(train_docs_labeled_no_neutral))\n",
    "save_pickle('../train_data_lebeled_no_neutral.pickle', train_data_labeled_no_neutral)\n",
    "save_pickle('../train_docs_lebeled_no_neutral.pickle', train_docs_labeled_no_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_df(df):\n",
    "    df = pd.DataFrame(columns=['tokens', 'emotion'])\n",
    "    for idx, row in enumerate(df):\n",
    "        df.loc[len(df)] = {\n",
    "        'tokens': row[0],\n",
    "        'emotion': row[1]\n",
    "        }\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[\"75분짜리, 결코  길다할 수없는 애니메이션 한편에 이정도 탄탄한 내용을 담을 ...</td>\n",
       "      <td>무섭다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[\"난무하는 욕설이나 더러운 말들, OO란 단어의 반복까진 참았다. 남녀가 첫만남부...</td>\n",
       "      <td>역겹다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[\"내가 실제 그장소에있다고 몰입 찐하게 하면 역대급으로 무섭고 ,영화가 이번씬은 ...</td>\n",
       "      <td>무섭다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"너무 가슴아픈 일이라 펑펑울면 어쩌나 걱정을 많이 했는데, 담담하게 볼수 있었습...</td>\n",
       "      <td>슬프다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"너에게 감동을 줄테니,너는 나에게 호주머니의 지갑을 열어달라고 울부짖고있음\"]</td>\n",
       "      <td>기쁘다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence emotion\n",
       "0  [\"75분짜리, 결코  길다할 수없는 애니메이션 한편에 이정도 탄탄한 내용을 담을 ...     무섭다\n",
       "1  [\"난무하는 욕설이나 더러운 말들, OO란 단어의 반복까진 참았다. 남녀가 첫만남부...     역겹다\n",
       "2  [\"내가 실제 그장소에있다고 몰입 찐하게 하면 역대급으로 무섭고 ,영화가 이번씬은 ...     무섭다\n",
       "3  [\"너무 가슴아픈 일이라 펑펑울면 어쩌나 걱정을 많이 했는데, 담담하게 볼수 있었습...     슬프다\n",
       "4      [\"너에게 감동을 줄테니,너는 나에게 호주머니의 지갑을 열어달라고 울부짖고있음\"]     기쁘다"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_no_neutral = pd.DataFrame(train_data_labeled_no_neutral, \n",
    "                                  columns=['sentence', 'emotion'])\n",
    "df_data_no_neutral.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[\"/Punctuation, 75/Number, 분/Noun, 짜다/Verb, ,/...</td>\n",
       "      <td>무섭다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[\"/Punctuation, 난무/Noun, 하다/Verb, 욕설/Noun, 이나/...</td>\n",
       "      <td>역겹다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[\"/Punctuation, 내/Noun, 가/Josa, 실제/Noun, 그/Det...</td>\n",
       "      <td>무섭다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"/Punctuation, 너무/Noun, 가슴/Noun, 아프다/Adjectiv...</td>\n",
       "      <td>슬프다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"/Punctuation, 너/Noun, 에게/Josa, 감동/Noun, 을/Jo...</td>\n",
       "      <td>기쁘다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens emotion\n",
       "0  [\"/Punctuation, 75/Number, 분/Noun, 짜다/Verb, ,/...     무섭다\n",
       "1  [\"/Punctuation, 난무/Noun, 하다/Verb, 욕설/Noun, 이나/...     역겹다\n",
       "2  [\"/Punctuation, 내/Noun, 가/Josa, 실제/Noun, 그/Det...     무섭다\n",
       "3  [\"/Punctuation, 너무/Noun, 가슴/Noun, 아프다/Adjectiv...     슬프다\n",
       "4  [\"/Punctuation, 너/Noun, 에게/Josa, 감동/Noun, 을/Jo...     기쁘다"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_docs_no_neutral = pd.DataFrame(train_docs_labeled_no_neutral, \n",
    "                                  columns=['tokens', 'emotion'])\n",
    "df_docs_no_neutral.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data_no_neutral.to_csv('../train_data_labeled_no_neutral.txt', encoding='utf-8', index=False)\n",
    "df_docs_no_neutral.to_csv('../train_docs_labeled_no_neutral.txt', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_data_labeled_joy = pd.DataFrame(train_data_labeled_joy, columns=['sentence'])\n",
    "df_train_data_labeled_anger = pd.DataFrame(train_data_labeled_anger, columns=['sentence'])\n",
    "df_train_data_labeled_disgust = pd.DataFrame(train_data_labeled_disgust, columns=['sentence'])\n",
    "df_train_data_labeled_sadness = pd.DataFrame(train_data_labeled_sadness, columns=['sentence'])\n",
    "df_train_data_labeled_fear = pd.DataFrame(train_data_labeled_fear, columns=['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_docs_labeled_joy = [row for row in train_docs_labeled if row[1] == '기쁘다']\n",
    "train_docs_labeled_anger = [row for row in train_docs_labeled if row[1] == '화나다']\n",
    "train_docs_labeled_disgust = [row for row in train_docs_labeled if row[1] == '역겹다']\n",
    "train_docs_labeled_sadness = [row for row in train_docs_labeled if row[1] == '슬프다']\n",
    "train_docs_labeled_fear = [row for row in train_docs_labeled if row[1] == '무섭다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_docs_labeled_joy = pd.DataFrame(train_docs_labeled_joy, columns=['tokens', 'emotion'])\n",
    "df_train_docs_labeled_anger = pd.DataFrame(train_docs_labeled_anger, columns=['tokens', 'emotion'])\n",
    "df_train_docs_labeled_disgust = pd.DataFrame(train_docs_labeled_disgust, columns=['tokens', 'emotion'])\n",
    "df_train_docs_labeled_sadness = pd.DataFrame(train_docs_labeled_sadness, columns=['tokens', 'emotion'])\n",
    "df_train_docs_labeled_fear = pd.DataFrame(train_docs_labeled_fear, columns=['tokens', 'emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_data_list = [df_train_data_labeled_joy, \n",
    "                      df_train_data_labeled_anger,\n",
    "                      df_train_data_labeled_disgust,\n",
    "                      df_train_data_labeled_sadness,\n",
    "                      df_train_data_labeled_fear]\n",
    "\n",
    "df_train_docs_list = [df_train_docs_labeled_joy, \n",
    "                      df_train_docs_labeled_anger,\n",
    "                      df_train_docs_labeled_disgust,\n",
    "                      df_train_docs_labeled_sadness,\n",
    "                      df_train_docs_labeled_fear]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_csv(directory, obj):\n",
    "    obj.to_csv(directory, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_train_data_list)):\n",
    "    to_csv('../train_data_labeled_{}.txt'.format(list(emotion_pair.keys())[i]), \n",
    "           df_train_data_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(df_train_data_list)):\n",
    "    to_csv('../train_docs_labeled_{}.txt'.format(list(emotion_pair.keys())[i]), \n",
    "           df_train_docs_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필터링 이후 데이터 개수:\n",
      "77601 40201 20311 20691 10615\n"
     ]
    }
   ],
   "source": [
    "train_data_joy_filtered = read_data('../train_data_labeled_joy.txt')\n",
    "train_data_anger_filtered = read_data('../train_data_labeled_anger.txt')\n",
    "train_data_disgust_filtered = read_data('../train_data_labeled_disgust.txt')\n",
    "train_data_sadness_filtered = read_data('../train_data_labeled_sadness.txt')\n",
    "train_data_fear_filtered = read_data('../train_data_labeled_fear.txt')\n",
    "\n",
    "print(\"필터링 이후 데이터 개수:\")\n",
    "print(len(train_data_joy_filtered), len(train_data_anger_filtered), \n",
    "      len(train_data_disgust_filtered), len(train_data_sadness_filtered), \n",
    "      len(train_data_fear_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2650\n",
      "812\n",
      "65\n",
      "-4\n",
      "2005\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data_labeled_joy) - len(train_data_joy_filtered))\n",
    "print(len(train_data_labeled_anger) - len(train_data_anger_filtered))\n",
    "print(len(train_data_labeled_disgust) - len(train_data_disgust_filtered))\n",
    "print(len(train_data_labeled_sadness) - len(train_data_sadness_filtered))\n",
    "print(len(train_data_labeled_fear) - len(train_data_fear_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2018/04/29 22:05**\n",
    "- 데이터 필터링 (레이블과 맞지 않는 데이터 제거)\n",
    "- 필터링 이후 데이터 개수:\n",
    "    - 기쁘다: 77601 (-2650)\n",
    "    - 화나다: 40201 (-812)\n",
    "    - 역겹다: 20311 (-65)\n",
    "    - 슬프다: 20691 (+4)\n",
    "    - 무섭다: 10615 (-2005)\n",
    "    \n",
    "\n",
    "- `tokenizer`의 한계로 부정문, 조사에 따른 변화를 (교착어의 특성을) 제대로 잡아내지 못함\n",
    "- 특히 `norm`과 `stem` 모두 `True`로 설정해서\n",
    "    - `tokenize('무서워')` --> '무섭다/Adjective'\n",
    "    - `tokenize('무섭냐')` --> '무섭다/Adjective'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def lst_of_lst_rm_duplicates(data):\n",
    "    data = list(row for row, _ in itertools.groupby(data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77601\n",
      "40201\n",
      "20311\n",
      "20691\n",
      "10615\n"
     ]
    }
   ],
   "source": [
    "train_data_filtered_list = [train_data_joy_filtered, \n",
    "                            train_data_anger_filtered, \n",
    "                            train_data_disgust_filtered, \n",
    "                            train_data_sadness_filtered, \n",
    "                            train_data_fear_filtered]\n",
    "\n",
    "for data in train_data_filtered_list:\n",
    "    lst_of_lst_rm_duplicates(data)\n",
    "    print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_joy_filtered = random.sample(train_data_joy_filtered, 7600)\n",
    "test_data_anger_filtered = random.sample(train_data_anger_filtered, 4020)\n",
    "test_data_disgust_filtered = random.sample(train_data_disgust_filtered, 2030)\n",
    "test_data_sadness_filtered = random.sample(train_data_sadness_filtered, 2070)\n",
    "test_data_fear_filtered = random.sample(train_data_fear_filtered, 1060)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 데이터 레이블 태깅\n",
    "\n",
    "train_data_joy_filtered = [(row, 0) for row in train_data_joy_filtered]\n",
    "train_data_anger_filtered = [(row, 1) for row in train_data_anger_filtered]\n",
    "train_data_disgust_filtered = [(row, 2) for row in train_data_disgust_filtered]\n",
    "train_data_sadness_filtered = [(row, 3) for row in train_data_sadness_filtered]\n",
    "train_data_fear_filtered = [(row, 4) for row in train_data_fear_filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(doc, stopwords):\n",
    "    \"\"\"\n",
    "    Remove stopwords from doc.\n",
    "    :param doc: Document to remove stopwords\n",
    "    :param stopwords: stopwords corpus\n",
    "    :return: documents with removed stopwords\n",
    "    \"\"\"\n",
    "    stopwords_removed = []\n",
    "    for token in doc:\n",
    "        if not token in stopwords:\n",
    "            stopwords_removed.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_weights(matrix, x1, x2):\n",
    "    \"\"\"\n",
    "    Get normalized weights matrix. Used for np.dot(TDM, weights).\n",
    "    :param matrix: similarity matrix (euclidean or cosine)\n",
    "    :param x1: rows\n",
    "    :param x2: cols\n",
    "    :return: normalized weights matrix\n",
    "    \"\"\"\n",
    "    distance = matrix[x1, x2] ** 2\n",
    "    variance = np.var(matrix)\n",
    "    weights = np.exp(-(distance / (2 * variance ** 2)))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = read_data('../reviews/all_contents_03.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 33s, sys: 1.34 s, total: 4min 34s\n",
      "Wall time: 4min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_docs = [tokenize(row[0]) for row in train_data]\n",
    "train_docs_no_pos = [remove_pos(docs) for docs in train_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['힘내다/Verb',\n",
      " '모두/Noun',\n",
      " '들/Suffix',\n",
      " '!!/Punctuation',\n",
      " '진실/Noun',\n",
      " '을/Josa',\n",
      " '밝히다/Verb',\n",
      " '!!/Punctuation']\n",
      "\n",
      "['힘내다', '모두', '들', '!!', '진실', '을', '밝히다', '!!']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(train_docs[-100])\n",
    "print('')\n",
    "pprint(train_docs_no_pos[-100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: NMSC>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "tokens = [t for d in train_docs for t in d]\n",
    "text = nltk.Text(tokens, name=\"NMSC\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3931769\n",
      "63779\n",
      "[('./Punctuation', 115283),\n",
      " ('하다/Verb', 93744),\n",
      " ('영화/Noun', 90605),\n",
      " ('이/Josa', 71434),\n",
      " ('보다/Verb', 65114),\n",
      " ('의/Josa', 51345),\n",
      " ('에/Josa', 49016),\n",
      " ('../Punctuation', 47244),\n",
      " ('가/Josa', 46659),\n",
      " ('을/Josa', 42707)]\n"
     ]
    }
   ],
   "source": [
    "print(len(text.tokens))\n",
    "print(len(set(text.tokens)))\n",
    "pprint(text.vocab().most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 58s, sys: 3.74 s, total: 17min 2s\n",
      "Wall time: 4min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(train_docs, size=300, workers=4, min_count=10, iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_pickle('../train_docs_0426.pickle', train_docs)\n",
    "save_pickle('../train_docs_no_pos_0426.pickle', train_docs_no_pos)\n",
    "save_pickle('../nltk_text_0426.pickle', text)\n",
    "model.save('../model/review_word2vec_20180426')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter; twitter = Twitter()\n",
    "from gensim.models import word2vec\n",
    "train_docs = load_pickle('../train_docs_0426.pickle')\n",
    "train_docs_no_pos = load_pickle('../train_docs_no_pos_0426.pickle')\n",
    "text = load_pickle('../nltk_text_0426.pickle')\n",
    "model = word2vec.Word2Vec.load('../model/review_word2vec_20180426')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['기쁘다/Adjective',\n",
       " '화나다/Verb',\n",
       " '역겹다/Adjective',\n",
       " '슬프다/Adjective',\n",
       " '무섭다/Adjective']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_pair = {'joy': '기쁘다', 'anger': '화나다', \n",
    "                'disgust': '역겹다', 'sadness': '슬프다', 'fear': '무섭다'}\n",
    "emotion_ko_list = list(emotion_pair.values())\n",
    "emotion_ko_dic = {\n",
    "    '기쁘다': 0, \n",
    "    '화나다': 1, \n",
    "    '역겹다': 2,\n",
    "    '슬프다': 3,\n",
    "    '무섭다': 4,\n",
    "}\n",
    "emotion_ko_list = [tokenize(row) for row in emotion_ko_list]\n",
    "emotion_ko_list = [row[0] for row in emotion_ko_list]\n",
    "emotion_ko_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = ['귀신/Noun', '깜놀/Noun', '놀라다/Verb', '멀미/Noun', '놀램/Noun',\n",
    "       '지르다/Verb', '갑툭튀/Noun', '지름/Noun', '놀람/Noun', '나쁘다/Adjective',\n",
    "       '비명/Noun', '튀어나오다/Verb', '소리내다/Verb', '오지/Noun', '울다/Verb',\n",
    "       '놀랬/Noun', '무섭다/Adjective', '깜짝/Noun', '놀란/Noun', '놀랬어/Noun', \n",
    "       '깜짝깜짝/Adverb', '섬뜩/Adverb', '소름/Noun', '돋다/Adjective']\n",
    "\n",
    "li = remove_pos(tmp)\n",
    "\n",
    "def get_similar_words(docs):\n",
    "    li_new = []\n",
    "    for word in li:\n",
    "        try:\n",
    "            token = model.wv.most_similar(tokenize(twitter, word)[0], topn=8)\n",
    "            if token not in li_new:\n",
    "                li_new.append(token)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return li_new\n",
    "\n",
    "li_new = get_similar_words(li)\n",
    "for line in li_new:\n",
    "    for w in line:\n",
    "        if w[0] not in tmp:\n",
    "            tmp.append(w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오름/Noun',\n",
      " '비명/Noun',\n",
      " '뿜었/Noun',\n",
      " '잼슴/Noun',\n",
      " '웃다/Verb',\n",
      " 'Ost/Alpha',\n",
      " '드럼/Noun',\n",
      " '놀라다/Verb',\n",
      " '오지/Noun',\n",
      " '빙의/Noun']\n"
     ]
    }
   ],
   "source": [
    "pprint(list(fear_list)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('무섭다/Adjective', 0.6415336728096008),\n",
       " ('애나벨/Noun', 0.36187833547592163),\n",
       " ('귀신/Noun', 0.3564368188381195),\n",
       " ('깜놀/Noun', 0.3522293269634247),\n",
       " ('놀램/Noun', 0.3495345115661621)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('무서움/Noun', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data_comma(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = [line.split(',') for line in f.read().splitlines()]\n",
    "        data = data[1:]\n",
    "    return data\n",
    "\n",
    "# emotion_corpus = read_data_comma('../emotion_corpus_labeled.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>기쁘다/Adjective</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>감동/Noun</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>감사/Noun</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>고맙다/Adjective</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>괜찮다/Adjective</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           token  emotion\n",
       "0  기쁘다/Adjective        0\n",
       "1        감동/Noun        0\n",
       "2        감사/Noun        0\n",
       "3  고맙다/Adjective        0\n",
       "4  괜찮다/Adjective        0"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_corpus = pd.read_csv('../emotion_corpus_labeled.txt', encoding='utf-8', sep=',')\n",
    "emotion_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 152, 137, 93, 88)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joy_list = emotion_corpus[emotion_corpus['emotion']==0].values.tolist()\n",
    "anger_list = emotion_corpus[emotion_corpus['emotion']==1].values.tolist()\n",
    "disgust_list = emotion_corpus[emotion_corpus['emotion']==2].values.tolist()\n",
    "sadness_list = emotion_corpus[emotion_corpus['emotion']==3].values.tolist()\n",
    "fear_list = emotion_corpus[emotion_corpus['emotion']==4].values.tolist()\n",
    "\n",
    "joy_list = [row[0] for row in joy_list]\n",
    "anger_list = [row[0] for row in anger_list]\n",
    "disgust_list = [row[0] for row in disgust_list]\n",
    "sadness_list = [row[0] for row in sadness_list]\n",
    "fear_list = [row[0] for row in fear_list]\n",
    "\n",
    "len(joy_list), len(anger_list), len(disgust_list), len(sadness_list), len(fear_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.1 s, sys: 86.8 ms, total: 29.2 s\n",
      "Wall time: 29.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_docs_labeled = []\n",
    "\n",
    "for row in train_docs:\n",
    "    joy_score = 0\n",
    "    anger_score = 0\n",
    "    disgust_score = 0\n",
    "    sadness_score = 0\n",
    "    fear_score = 0\n",
    "    all_scores = {}\n",
    "    \n",
    "    all_scores['기쁘다'] = joy_score\n",
    "    all_scores['화나다'] = anger_score\n",
    "    all_scores['역겹다'] = disgust_score\n",
    "    all_scores['슬프다'] = sadness_score\n",
    "    all_scores['무섭다'] = fear_score\n",
    "    \n",
    "    for _, token in enumerate(row):\n",
    "            \n",
    "        if token in joy_list:\n",
    "            joy_score += 1\n",
    "        elif token in anger_list:\n",
    "            anger_score += 1\n",
    "        elif token in disgust_list:\n",
    "            disgust_score += 1\n",
    "        elif token in sadness_list:\n",
    "            sadness_score += 1\n",
    "        elif token in fear_list:\n",
    "            fear_score += 1\n",
    "        \n",
    "        all_scores['기쁘다'] += joy_score\n",
    "        all_scores['화나다'] += anger_score\n",
    "        all_scores['역겹다'] += disgust_score\n",
    "        all_scores['슬프다'] += sadness_score\n",
    "        all_scores['무섭다'] += fear_score   \n",
    "        \n",
    "        label = max(all_scores, key=lambda key: all_scores[key])\n",
    "    \n",
    "    if all_scores[label] == 0:\n",
    "        label = '중립'\n",
    "        train_docs_labeled.append((row, label))\n",
    "#     elif all_scores['기쁘다'] == all_scores['화나다'] == all_scores['역겹다'] == all_scores['슬프다'] == all_scores['무섭다']:\n",
    "#         label = '중립'\n",
    "#         train_docs_labeled.append((row, label))\n",
    "    else:\n",
    "        train_docs_labeled.append((row, label))   \n",
    "#     print(all_scores)\n",
    "#     print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'기쁘다': 69354,\n",
       "         '무섭다': 14219,\n",
       "         '슬프다': 17737,\n",
       "         '역겹다': 18923,\n",
       "         '중립': 104237,\n",
       "         '화나다': 28377})"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labeled = [row[1] for row in train_docs_labeled]\n",
    "\n",
    "from collections import Counter\n",
    "label_count = Counter(all_labeled)\n",
    "label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104237\n"
     ]
    }
   ],
   "source": [
    "train_docs_labeld_neutral = []\n",
    "for row in train_docs_labeled:\n",
    "    if row[1] == '중립':\n",
    "        train_docs_labeld_neutral.append(row)\n",
    "\n",
    "indicies = []\n",
    "\n",
    "for idx, row in enumerate(train_docs_labeled):\n",
    "    if row[1] == '중립':\n",
    "        indicies.append(idx)\n",
    "print(len(indicies))\n",
    "train_docs_neutral = np.array(train_docs)[indicies].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 37s, sys: 1.12 s, total: 4min 38s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_neutral = word2vec.Word2Vec(train_docs_neutral, size=300, workers=4, min_count=10, iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_pickle('../train_docs_neutral_0427.pickle', train_docs_neutral)\n",
    "model_neutral.save('../model/review_word2vec_neutral_20180427')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(model_neutral.wv.vocab.keys()))\n",
    "df.to_csv('../neutral_corpus.txt', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "text_neutral = nltk.Text(train_docs_neutral, name=\"NEUTRAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('불쌍하다/Adjective', 0.2824586033821106),\n",
       " ('비위/Noun', 0.2561807632446289),\n",
       " ('감수성/Noun', 0.23796811699867249),\n",
       " ('신앙/Noun', 0.23324984312057495),\n",
       " ('무미건조/Noun', 0.22985348105430603),\n",
       " ('연약/Noun', 0.22469256818294525),\n",
       " ('가출/Noun', 0.22458600997924805),\n",
       " ('한심하다/Adjective', 0.22446684539318085),\n",
       " ('멘탈/Noun', 0.2216242253780365),\n",
       " ('멀미/Noun', 0.22064091265201569),\n",
       " ('참담/Noun', 0.21878448128700256),\n",
       " ('나약/Noun', 0.2170790433883667),\n",
       " ('부끄러움/Noun', 0.21612493693828583),\n",
       " ('피비/Noun', 0.2144235074520111),\n",
       " ('노망/Noun', 0.2143237143754959),\n",
       " ('하앍/Noun', 0.2133876234292984),\n",
       " ('지못미/Noun', 0.21295066177845),\n",
       " ('짜증/Noun', 0.21245987713336945),\n",
       " ('전능/Noun', 0.21142469346523285),\n",
       " ('사랑스럽다/Adjective', 0.2111159861087799),\n",
       " ('이기심/Noun', 0.2098180055618286),\n",
       " ('화도/Noun', 0.20951983332633972),\n",
       " (',..../Punctuation', 0.20921377837657928),\n",
       " ('신격화/Noun', 0.20774462819099426),\n",
       " ('애처/Noun', 0.2071099728345871),\n",
       " ('화병/Noun', 0.20569823682308197),\n",
       " ('미개/Noun', 0.20228713750839233),\n",
       " ('투철/Noun', 0.20159800350666046),\n",
       " ('현기증/Noun', 0.19893409311771393),\n",
       " ('희롱/Noun', 0.19653312861919403),\n",
       " ('그러므로/Conjunction', 0.1956866979598999),\n",
       " ('연민/Noun', 0.1953163743019104),\n",
       " ('초등생/Noun', 0.19491873681545258),\n",
       " ('하하/Noun', 0.19467484951019287),\n",
       " ('귀엽다/Adjective', 0.19446422159671783),\n",
       " ('향기/Noun', 0.1938152015209198),\n",
       " ('재희/Noun', 0.19377447664737701),\n",
       " ('구타/Noun', 0.19374848902225494),\n",
       " ('바랬는데/Noun', 0.1933637410402298),\n",
       " ('무식/Noun', 0.1931779682636261),\n",
       " ('젠장/Noun', 0.1922784149646759),\n",
       " ('서영이/Noun', 0.19205138087272644),\n",
       " ('순애보/Noun', 0.19198906421661377),\n",
       " ('제타존스/Noun', 0.1916290670633316),\n",
       " ('안녕/Noun', 0.19117958843708038),\n",
       " ('우월/Noun', 0.19090630114078522),\n",
       " ('에고/Noun', 0.1907762885093689),\n",
       " ('소름/Noun', 0.19034580886363983),\n",
       " ('구역질/Noun', 0.18944190442562103),\n",
       " ('가려지다/Verb', 0.1893361508846283)]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('가엾다/Adjective', topn=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
